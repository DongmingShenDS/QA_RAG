{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c711a27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_notes import QA_PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec40928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading chroma db\n"
     ]
    }
   ],
   "source": [
    "chat = QA_PDF(chat_model=\"open-mixtral-8x7b\", emb_model=\"mistral-embed\")\n",
    "chat.read_directory(\"pdfs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ecb7651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'what is page attention and why is it useful in large language model inference?',\n",
       " 'context': [Document(page_content='10Conclusion\\nThis paper proposes PagedAttention, a new attention algo-\\nrithm that allows attention keys and values to be stored\\nin non-contiguous paged memory, and presents vLLM, a\\nhigh-throughput LLM serving system with e\\uffffcient mem-\\nory management enabled by PagedAttention. Inspired by\\noperating systems, we demonstrate how established tech-\\nniques, such as virtual memory and copy-on-write, can be\\nadapted to e\\uffffciently manage KV cache and handle various', metadata={'page': 2, 'source': 'pdfs/2309.06180v1.pdf'}),\n",
       "  Document(page_content='and shrinks dynamically. When managed ine\\uffffciently, this\\nmemory can be signi\\uffffcantly wasted by fragmentation and\\nredundant duplication, limiting the batch size. To address\\nthis problem, we propose PagedAttention, an attention al-\\ngorithm inspired by the classical virtual memory and pag-\\ning techniques in operating systems. On top of it, we build\\nvLLM, an LLM serving system that achieves (1) near-zero\\nwaste in KV cache memory and (2) \\uffffexible sharing of KV', metadata={'page': 0, 'source': 'pdfs/2309.06180v1.pdf'}),\n",
       "  Document(page_content='Orca [ 60] and PagedAttention in vLLM are complementary\\ntechniques: While both systems aim to increase the GPU\\nutilization and hence the throughput of LLM serving, Orca\\nachieves it by scheduling and interleaving the requests so\\nthat more requests can be processed in parallel, while vLLM\\nis doing so by increasing memory utilization so that the\\nworking sets of more requests \\ufffft into memory. By reducing\\nmemory fragmentation and enabling sharing, vLLM runs', metadata={'page': 1, 'source': 'pdfs/2309.06180v1.pdf'}),\n",
       "  Document(page_content='E\\uffffcient Memory Management for Large Language\\nModel Serving with PagedA\\uffffention\\nWoosuk Kwon1,⇤Zhuohan Li1,⇤Siyuan Zhuang1Ying Sheng1,2Lianmin Zheng1Cody Hao Yu3\\nJoseph E. Gonzalez1Hao Zhang4Ion Stoica1\\n1UC Berkeley2Stanford University3Independent Researcher4UC San Diego\\nAbstract\\nHigh throughput serving of large language models (LLMs)\\nrequires batching su\\uffffciently many requests at a time. How-\\never, existing systems struggle because the key-value cache\\n(KV cache) memory for each request is huge and grows', metadata={'page': 0, 'source': 'pdfs/2309.06180v1.pdf'}),\n",
       "  Document(page_content='paging is e\\uffffective for managing the KV cache in LLM serving\\nbecause the workload requires dynamic memory allocation\\n(since the output length is not known a priori) and its perfor-\\nmance is bound by the GPU memory capacity. However, this\\ndoes not generally hold for every GPU workload. For exam-\\nple, in DNN training, the tensor shapes are typically static,\\nand thus memory allocation can be optimized ahead of time.\\nFor another example, in serving DNNs that are not LLMs,', metadata={'page': 1, 'source': 'pdfs/2309.06180v1.pdf'})],\n",
       " 'answer': 'PagedAttention is a new attention algorithm inspired by classical virtual memory and paging techniques in operating systems. It allows attention keys and values to be stored in non-contiguous paged memory, and is used in vLLM, a high-throughput LLM serving system. This algorithm is effective for managing the KV cache in LLM serving because it reduces memory fragmentation and enables sharing, allowing for near-zero waste in KV cache memory and flexible sharing of KV. This increases memory utilization, which in turn increases the throughput of LLM serving.'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.ask(\"what is page attention and why is it useful in large language model inference?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f99d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
